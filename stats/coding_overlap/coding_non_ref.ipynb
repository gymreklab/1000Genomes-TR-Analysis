{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from cyvcf2 import VCF\n",
    "import pandas as pd\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading coding TR coordinates ###\n",
    "coordinates = pd.read_csv(\"TR_intersect.txt\", sep = \"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_nr_per_sample_coding = defaultdict(int)\n",
    "both_nr_per_sample_coding = defaultdict(int)\n",
    "\n",
    "for chrom in range(1,23):\n",
    "    vcf_addr = f\"/expanse/projects/gymreklab/helia/ensembl/experiments/coding_regions/heterozygosity/coding_calls_{chrom}.vcf.gz\"\n",
    "    vcf = VCF(vcf_addr)\n",
    "    samples = vcf.samples\n",
    "    for record in vcf:\n",
    "        if int(record.POS) in list(coordinates[1]) and int(record.INFO['END']) in list(coordinates[2]):\n",
    "            genotypes = record.genotypes\n",
    "            for i in range(len(samples)):\n",
    "                sample = samples[i]\n",
    "                genotype = genotypes[i]\n",
    "                if genotype[0] == -1:\n",
    "                    continue\n",
    "                if genotype[0] != 0 and genotype[1] != 0:\n",
    "                    both_nr_per_sample_coding[sample] += 1\n",
    "                    continue\n",
    "                if genotype[0] != 0 or genotype[1] != 0:\n",
    "                    one_nr_per_sample_coding[sample] += 1\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_nr_per_sample_whole = defaultdict(int)\n",
    "both_nr_per_sample_whole = defaultdict(int)\n",
    "\n",
    "\n",
    "for i in range(1,23):\n",
    "    gt_file = f\"/expanse/projects/gymreklab/helia/ensembl/experiments/stats/info/gt_chr{i}.txt\"\n",
    "    with open(gt_file) as f:\n",
    "        for line in f:\n",
    "            line = line.split()\n",
    "            GT = line[3].split(\"/\")\n",
    "            if GT[0] == \".\":\n",
    "                continue\n",
    "            if GT[0] != '0' and GT[1] != '0':\n",
    "                both_nr_per_sample_whole[line[2]] += 1\n",
    "                continue\n",
    "            if GT[0] != '0' or GT[1] != '0':\n",
    "                one_nr_per_sample_whole[line[2]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merging all data ###\n",
    "one_nr_per_sample_coding_df = pd.DataFrame(one_nr_per_sample_coding.items())\n",
    "both_nr_per_sample_coding_df = pd.DataFrame(both_nr_per_sample_coding.items())\n",
    "\n",
    "one_nr_per_sample_whole_df = pd.DataFrame(one_nr_per_sample_whole.items())\n",
    "both_nr_per_sample_whole_df = pd.DataFrame(both_nr_per_sample_whole.items())\n",
    "\n",
    "all_df_list = [one_nr_per_sample_coding_df, both_nr_per_sample_coding_df, \n",
    "               one_nr_per_sample_whole_df, both_nr_per_sample_whole_df]\n",
    "\n",
    "all_df = reduce(lambda  left,right: pd.merge(left,right,on=[0],\n",
    "                                            how='inner'), all_df_list)\n",
    "\n",
    "all_df.columns = ['sample', 'coding_one_allele_non_ref', 'coding_two_alleles_non_ref',\n",
    "                  'wg_one_allele_non_ref', 'wg_two_alleles_non_ref']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Loading pedigree ####\n",
    "\n",
    "pedigree = pd.read_csv(\"/expanse/projects/gymreklab/helia/TR_1000G/1000G.ped\", delim_whitespace=True)\n",
    "pedigree = pedigree[['SampleID', 'Population', 'Superpopulation']]\n",
    "\n",
    "H3Africa_names = pd.read_csv(\"/expanse/projects/gymreklab/helia/H3Africa/names/H3A_Baylor_sample_country.txt\", header=None, delim_whitespace=True)\n",
    "H3Africa_names['Superpopulation'] = \"H3Africa\"\n",
    "H3Africa_names.columns = ['SampleID', 'Population', 'Superpopulation']\n",
    "\n",
    "all_samples = pd.concat([pedigree, H3Africa_names])\n",
    "all_samples\n",
    "    \n",
    "all_df_family = pd.merge(all_df, all_samples, left_on = \"sample\",\n",
    "                         right_on = \"SampleID\")[['sample', 'Population', 'Superpopulation',\n",
    "                                                 'coding_one_allele_non_ref', 'coding_two_alleles_non_ref',\n",
    "                                                 'wg_one_allele_non_ref', 'wg_two_alleles_non_ref']]\n",
    "all_df_family\n",
    "all_df_family.to_csv(\"tables/Supplementary_Table_6.csv\", index = False, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183899.8861971831 184643.7090140845\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(all_df_family['wg_one_allele_non_ref']), np.mean(all_df_family['wg_two_alleles_non_ref']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158.94732394366198 128.78704225352112\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(all_df_family['coding_one_allele_non_ref']), np.mean(all_df_family['coding_two_alleles_non_ref']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
